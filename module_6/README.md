module_6

Проект 6. Выбираем авто выгодно

Цель проекта: 

Построение модели для прогнозирования стоимость автомобилей.

Задачи проекта:

- EDA предоставленных данных
- Создание моделей
- Подбор гиперпараметров
- Получение предсказанных моделью значений для участия в соревновании на Kaggle

Описание датасета:

Исходный датасет формировался на основе данных 2020 года. 
Дополнительно, в феврале 2022 года были спарсены дополнительные данные.

Что было сделано:

1) Проведен парсинг новых данных по предложениям автомобилей на вторичном рынке.
Из-за недостатка времени был сформирован код парсинга и с его помощью собраны данные 
о примерно 3600 предложениях автомобилей в Москве.
2) Проведена очитска данных.
Заполнены пропуски в категориальном признаке "education" с помощью функции 
пропорционального заполения пропусков в каждой категории.
В случае, где пропусков было очень много, а признак терять не хотелось, прменялся 
класс IterativeImputer библиотеки scikit-learn.
3) Перекодированы в числовые категориальные и бинарные признки.
4) Проанализированы значимости переменных.
5) Обучены модели: CatBoost, ExtraTreesRegressor, 
RandomForestRegressor, XGBRegressor.
6) Были определены оптимальные гиперпараметры для моделей: RandomForestRegressor, 
 GradientBoostingRegressor.
7) Был проведен анализ метрик различных моделей с помощью пакета Lazy Predict.
8) Было провдеден Стекинг моделей GradientBoostingRegressor, RandomForestRegressor,
ExtraTreesRegressor. В качестве метамодели использовалась линейная регрессия с дефолтными
параметрами.
9) Ресурсоемкие расчеты проводились на платформе DataSphere от Yandex.

Результаты:

95% времени заняли парсинг и очистка данных. Из-за этого, к сожалению, 
остальные разделы проекта пришлось "скомкать". 
Тем не менее некоторые моменты можно отметить.

1) CatBoost выдает лучшие метрики (RMSE и коэф.детерминации), нежели модели, 
рекомендуемые Lazy Predict. Однако, метрика МАРЕ у CatBoost получается ниже, 
чем у XGBRegressor.
2) Лучший результат дала модель на основе XGBRegressor: 12.91% на валидационной 
выборке и 13.12% на ЛБ в Kaggle.

Что можно улучшить:

1) Спарсить больше свежих данных (времени хватило примерно на 3600 записей).
2) Очистку начать с целевой переменной. Удаляем строки с пропусками в целевой 
переменной = удаляем потенциальный мусор в других переменных.
3) Поиграть с преобразованием переменных: логарифмирование, стандартизация, 
очистка от выбросов и пр.
4) Убрать мультиколлинеарность.
 